{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from open_flamingo.src.helpers import PerceiverResampler, PerceiverAttention, FeedForward, VisionTokenizer\n",
    "from einops_exts import rearrange_many\n",
    "# from open_flamingo.src.helpers import Forward, \n",
    "DIR = '/export/share/yutong/xgenmm/llamacpp_wd/siglip_kosmos_phi3_4k_instruct'\n",
    "ckpt = torch.load(DIR + '/xgenmm.projector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projector = PerceiverResampler(dim=1152, dim_inner=3072, depth=6, dim_head=96,heads=16,num_latents=128)\n",
    "# projector.load_state_dict(ckpt, strict=True)\n",
    "from torch import einsum, nn\n",
    "class MyPerceiverAttention(nn.Module):\n",
    "    def __init__(self, *, dim, dim_head=64, heads=8):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm_media = nn.LayerNorm(dim)\n",
    "        self.norm_latents = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x, latents, vision_attn_masks=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): image features\n",
    "                shape (b, T, n1, D)\n",
    "            latent (torch.Tensor): latent features\n",
    "                shape (b, T, n2, D)\n",
    "        \"\"\"\n",
    "        x = self.norm_media(x)\n",
    "        # print('latents:', latents.shape)\n",
    "        # print('before ln:', latents)\n",
    "        latents = self.norm_latents(latents)\n",
    "        # print('after ln:', latents)\n",
    "        # print(latents)\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(latents)\n",
    "        kv_input = torch.cat((x, latents), dim=-2) # TODO: Change the shape of vision attention mask according to this.\n",
    "        if vision_attn_masks is not None:\n",
    "            vision_attn_masks = torch.cat((vision_attn_masks, \n",
    "                                            torch.ones((latents.shape[0], latents.shape[-2]), dtype=latents.dtype, device=latents.device)),\n",
    "                                            dim=-1)\n",
    "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
    "        # print('q:', q.shape, 'k:', k.shape)\n",
    "        # print('q * self.scale:', q * self.scale)\n",
    "        q, k, v = rearrange_many((q, k, v), \"b t n (h d) -> b h t n d\", h=h)\n",
    "        q = q * self.scale\n",
    "        # print('q:', q.shape, 'k:', k.shape)\n",
    "        # print('q', q)\n",
    "        # attention\n",
    "        sim = einsum(\"... i d, ... j d  -> ... i j\", q, k)\n",
    "        # print('sim:', sim.shape)\n",
    "        # print('sim:', sim)\n",
    "        # Apply vision attention mask here.\n",
    "        # Reference: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention\n",
    "        if vision_attn_masks is not None:\n",
    "            attn_bias = torch.zeros((q.size(0), 1, 1, q.size(-2), k.size(-2)), dtype=q.dtype, device=q.device)\n",
    "            vision_attn_masks = repeat(vision_attn_masks, 'b n -> b 1 1 l n', l=q.size(-2))\n",
    "            attn_bias.masked_fill_(vision_attn_masks.logical_not(), float(\"-inf\"))\n",
    "            sim += attn_bias\n",
    "        # print('remove safe softmax')\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        # print('attn:', attn.shape)\n",
    "        # print('attn:', attn)\n",
    "        \n",
    "        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n",
    "        out = rearrange(out, \"b h t n d -> b t n (h d)\", h=h)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "torch.set_printoptions(precision=6)\n",
    "class MyModel(VisionTokenizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_inner=None,\n",
    "        depth=6,\n",
    "        dim_head=96,\n",
    "        heads=16,\n",
    "        num_latents=64,\n",
    "        ff_mult=4,\n",
    "    ):\n",
    "        if dim_inner is not None:\n",
    "            projection = nn.Linear(dim, dim_inner)\n",
    "        else:\n",
    "            projection = None\n",
    "            dim_inner = dim\n",
    "        super().__init__(dim_media=dim, num_tokens_per_media=num_latents)\n",
    "        self.projection = projection\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        print('use MyPerceiverAttention')\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        MyPerceiverAttention(\n",
    "                            dim=dim, dim_head=dim_head, heads=heads\n",
    "                        ),\n",
    "                        FeedForward(dim=dim, mult=ff_mult),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, up_layer_idx, vision_attn_masks=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): image features\n",
    "                shape (b, T, F, v, D)\n",
    "            vision_attn_masks (torch.Tensor): attention masks for padded visiont tokens (i.e., x)\n",
    "                shape (b, v)\n",
    "        Returns:\n",
    "            shape (b, T, n, D) where n is self.num_latents\n",
    "        \"\"\"\n",
    "        b, T, F, v = x.shape[:4]\n",
    "        x = rearrange(\n",
    "            x, \"b T F v d -> b T (F v) d\"\n",
    "        )  # flatten the frame and spatial dimensions\n",
    "\n",
    "        # blocks\n",
    "        # FIXME: extending query tokens proportional to the vision sequence length. Hard-coded as dfn5b token_len=729.\n",
    "        latents = self.latents\n",
    "        latents = repeat(latents, \"n d -> b T n d\", b=b, T=T)\n",
    "        for attn, ff in self.layers[up_layer_idx]:\n",
    "            latents = attn(x, latents, vision_attn_masks) + latents\n",
    "            latents = ff(latents) + latents\n",
    "        return self.projection(self.norm(latents)) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use MyPerceiverAttention\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector = MyModel(dim=1152, dim_inner=3072, depth=6, dim_head=96,heads=16,num_latents=128)\n",
    "projector.load_state_dict(ckpt, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 729, 1152]), torch.Size([1, 1, 128, 1152]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange, repeat\n",
    "x = torch.load('torch_input.pt')\n",
    "b, T, F, v = x.shape[:4]\n",
    "x = rearrange(\n",
    "    x, \"b T F v d -> b T (F v) d\"\n",
    ")  # flatten the frame and spatial dimensions\n",
    "\n",
    "# blocks\n",
    "# FIXME: extending query tokens proportional to the vision sequence length. Hard-coded as dfn5b token_len=729.\n",
    "latents = projector.latents\n",
    "latents = repeat(latents, \"n d -> b T n d\", b=b, T=T)\n",
    "x.shape, latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one layer\n",
      "one layer\n",
      "one layer\n",
      "one layer\n",
      "one layer\n",
      "one layer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.212549, -1.531190,  1.109906,  ..., -0.526986,  1.954799,\n",
       "            0.747433],\n",
       "          [-2.086961, -0.522794,  1.756205,  ..., -0.127052, -0.632777,\n",
       "           -0.725916],\n",
       "          [ 0.810579, -0.438765,  0.084244,  ...,  1.184662,  3.135023,\n",
       "           -0.504171],\n",
       "          ...,\n",
       "          [ 0.877389, -0.372569,  0.230555,  ...,  1.573029,  0.251383,\n",
       "           -0.565987],\n",
       "          [ 1.149897, -0.206728,  0.088850,  ...,  0.226263, -0.506940,\n",
       "            0.667587],\n",
       "          [-2.386264, -0.104232,  4.303236,  ...,  0.231421,  2.927466,\n",
       "            0.611564]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents = projector.latents\n",
    "latents = repeat(latents, \"n d -> b T n d\", b=b, T=T)\n",
    "for attn, ff in projector.layers[:7]:\n",
    "    print('one layer')\n",
    "    ffn_input = attn(x, latents, None) + latents\n",
    "    ffn_output = ff(ffn_input) + ffn_input\n",
    "    latents = ffn_output\n",
    "ffn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.051345,  0.106190, -0.329059,  ..., -0.076452, -0.646982,\n",
       "            0.387117],\n",
       "          [ 0.146720,  0.416739,  0.177801,  ...,  0.757915, -0.779272,\n",
       "            0.101562],\n",
       "          [-0.861851, -0.739894,  0.088086,  ..., -0.329879, -0.030024,\n",
       "            0.196694],\n",
       "          ...,\n",
       "          [-0.044654, -0.909024,  0.351772,  ...,  1.402903,  0.128508,\n",
       "            0.024843],\n",
       "          [ 0.564767,  1.046333,  0.147816,  ...,  0.293189,  0.838271,\n",
       "            0.520264],\n",
       "          [-0.476983,  0.052826, -0.849679,  ...,  0.401451, -0.028640,\n",
       "            0.563384]]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector.projection(projector.norm(ffn_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.125840, -1.152360, -0.250579,  ...,  0.702943, -0.667477,\n",
       "            0.125382],\n",
       "          [ 0.404710, -0.654932,  0.052124,  ...,  0.409271,  1.328065,\n",
       "           -1.037527],\n",
       "          [ 0.224494,  0.217863, -0.925740,  ..., -1.003946, -1.285568,\n",
       "           -0.940877],\n",
       "          ...,\n",
       "          [ 1.961059, -0.128769,  1.245687,  ..., -0.747905, -1.223406,\n",
       "           -0.803803],\n",
       "          [ 1.250931,  0.260638, -0.322367,  ...,  1.388404,  0.753187,\n",
       "            1.057841],\n",
       "          [-0.822385,  1.267051, -0.065255,  ...,  0.139525, -1.134014,\n",
       "           -0.970328]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn, ff = projector.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = attn(x, latents)\n",
    "ffn_input = outputs + latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.193154, -1.191534,  1.030737,  ..., -1.064739,  0.755063,\n",
       "            0.570516],\n",
       "          [-1.445420, -0.834621,  1.985458,  ...,  0.295841, -1.793584,\n",
       "           -1.009780],\n",
       "          [ 0.590621,  0.971060, -0.044593,  ...,  1.033780,  1.789779,\n",
       "            0.500850],\n",
       "          ...,\n",
       "          [-0.190476, -0.839463,  0.623966,  ...,  0.057086,  0.115040,\n",
       "           -0.028689],\n",
       "          [-0.572107,  0.195076, -2.075379,  ...,  0.128773,  0.362365,\n",
       "            0.636456],\n",
       "          [-0.542767, -0.468020,  2.784523,  ...,  0.120420,  1.266044,\n",
       "            0.393691]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.237422,  0.595619,  0.052967,  ...,  0.083949,  0.468008,\n",
       "           -0.169237],\n",
       "          [ 0.221389,  0.089346,  0.026736,  ...,  0.359654, -0.048257,\n",
       "            0.539953],\n",
       "          [-0.083690,  1.282188,  0.372275,  ...,  0.252378,  0.091954,\n",
       "            0.194415],\n",
       "          ...,\n",
       "          [ 0.075917, -0.148179, -0.169421,  ..., -0.169345, -0.004600,\n",
       "           -0.135732],\n",
       "          [-0.169918, -0.131036,  1.439505,  ..., -0.026955, -0.133199,\n",
       "           -0.160709],\n",
       "          [-0.151510,  0.193553, -0.015910,  ..., -0.002669, -0.125417,\n",
       "           -0.112417]]]], grad_fn=<GeluBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ff[2](ff[1](ff[0](ffn_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.237422,  0.595619,  0.052967,  ...,  0.083949,  0.468008,\n",
       "           -0.169237],\n",
       "          [ 0.221389,  0.089346,  0.026736,  ...,  0.359654, -0.048257,\n",
       "            0.539953],\n",
       "          [-0.083690,  1.282188,  0.372275,  ...,  0.252378,  0.091954,\n",
       "            0.194415],\n",
       "          ...,\n",
       "          [ 0.075917, -0.148179, -0.169421,  ..., -0.169345, -0.004600,\n",
       "           -0.135732],\n",
       "          [-0.169918, -0.131036,  1.439505,  ..., -0.026955, -0.133199,\n",
       "           -0.160709],\n",
       "          [-0.151510,  0.193553, -0.015910,  ..., -0.002669, -0.125417,\n",
       "           -0.112417]]]], grad_fn=<GeluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nn.GELU(approximate='tanh')(ff[1](ff[0](ffn_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.10968, -1.06094,  0.92263,  ..., -1.09242,  1.04648,  0.94565],\n",
       "          [-1.74872, -0.71779,  1.96360,  ...,  0.25901, -1.39720, -0.80547],\n",
       "          [ 0.32887,  0.59734, -0.12273,  ...,  1.13904,  2.14656,  0.41976],\n",
       "          ...,\n",
       "          [-0.10411, -0.80275,  0.60681,  ...,  0.20535, -0.04840, -0.15943],\n",
       "          [-0.26529, -0.23041, -1.59478,  ..., -0.28988,  0.21144,  0.10470],\n",
       "          [-1.21751, -0.48135,  2.93902,  ...,  0.41143,  1.67213,  0.36061]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ff(ffn_input) + ffn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(attn.to_q.state_dict()['weight'], 'w.pt')\n",
    "# torch.save(attn.norm_media.weight, 'ln_w.pt')\n",
    "# torch.save(attn.norm_media.bias, 'ln_b.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.1258, -1.1524, -0.2506,  ...,  0.7029, -0.6675,  0.1254],\n",
       "           [ 0.4047, -0.6549,  0.0521,  ...,  0.4093,  1.3281, -1.0375],\n",
       "           [ 0.2245,  0.2179, -0.9257,  ..., -1.0039, -1.2856, -0.9409],\n",
       "           ...,\n",
       "           [ 1.9611, -0.1288,  1.2457,  ..., -0.7479, -1.2234, -0.8038],\n",
       "           [ 1.2509,  0.2606, -0.3224,  ...,  1.3884,  0.7532,  1.0578],\n",
       "           [-0.8224,  1.2671, -0.0653,  ...,  0.1395, -1.1340, -0.9703]]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.1170, -1.1500, -0.2608,  ...,  0.6681, -0.6668,  0.1079],\n",
       "           [ 0.4050, -0.6630,  0.0482,  ...,  0.4039,  1.3266, -1.0374],\n",
       "           [ 0.2603,  0.2543, -0.9079,  ..., -0.9850, -1.2718, -0.9196],\n",
       "           ...,\n",
       "           [ 1.9527, -0.1003,  1.2426,  ..., -0.7054, -1.1713, -0.7581],\n",
       "           [ 1.2876,  0.2804, -0.3153,  ...,  1.4153,  0.7773,  1.0838],\n",
       "           [-0.7789,  1.3087, -0.0273,  ...,  0.1751, -1.0870, -0.9215]]]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.load('torch_input.pt')\n",
    "attn.norm_media(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((1152,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.norm_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.1175, -1.1435, -0.2604,  ...,  0.6734, -0.6686,  0.1078],\n",
       "           [ 0.4032, -0.6596,  0.0495,  ...,  0.4078,  1.3293, -1.0434],\n",
       "           [ 0.2587,  0.2519, -0.9096,  ..., -0.9890, -1.2751, -0.9250],\n",
       "           ...,\n",
       "           [ 1.9496, -0.1005,  1.2478,  ..., -0.7078, -1.1743, -0.7627],\n",
       "           [ 1.2851,  0.2778, -0.3151,  ...,  1.4249,  0.7788,  1.0887],\n",
       "           [-0.7796,  1.2996, -0.0262,  ...,  0.1776, -1.0898, -0.9269]]]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x - x.mean(-1, keepdim=True)) / torch.sqrt(x.var(-1, keepdim=True) + 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.1165, -1.1495, -0.2606,  ...,  0.6678, -0.6665,  0.1079],\n",
       "           [ 0.4048, -0.6628,  0.0482,  ...,  0.4037,  1.3260, -1.0369],\n",
       "           [ 0.2602,  0.2542, -0.9075,  ..., -0.9846, -1.2713, -0.9192],\n",
       "           ...,\n",
       "           [ 1.9518, -0.1003,  1.2421,  ..., -0.7051, -1.1708, -0.7578],\n",
       "           [ 1.2870,  0.2803, -0.3152,  ...,  1.4147,  0.7770,  1.0833],\n",
       "           [-0.7785,  1.3082, -0.0273,  ...,  0.1750, -1.0865, -0.9211]]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x - x.mean(-1, keepdim=True)) / torch.sqrt(x.var(-1, keepdim=True) + 1e-5) * attn.norm_media.weight + attn.norm_media.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma (64,)\n",
      "beta (64,)\n",
      "mean (10, 32, 32, 1)\n",
      "variance (10, 32, 32, 1)\n",
      "(10, 32, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Initialize LayerNorm.\n",
    "        \n",
    "        Args:\n",
    "        normalized_shape (tuple): The shape of the normalized dimensions (usually the last dimensions).\n",
    "        eps (float): Small epsilon value to avoid division by zero.\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(normalized_shape)\n",
    "        print('gamma', self.gamma.shape)\n",
    "        self.beta = np.zeros(normalized_shape)\n",
    "        print('beta', self.beta.shape)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Apply layer normalization to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "        x (numpy.ndarray): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: The layer-normalized tensor.\n",
    "        \"\"\"\n",
    "        # Compute the mean and variance along the specified dimensions\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        print('mean', mean.shape)\n",
    "        variance = np.var(x, axis=-1, keepdims=True)\n",
    "        print('variance', variance.shape)\n",
    "        \n",
    "        # Normalize\n",
    "        normalized_x = (x - mean) / np.sqrt(variance + self.eps)\n",
    "        \n",
    "        # Apply affine transformation\n",
    "        normalized_x = self.gamma * normalized_x + self.beta\n",
    "        \n",
    "        return normalized_x\n",
    "\n",
    "# Example usage\n",
    "normalized_shape = (64,)  # Assuming normalization over the last dimension of size 64\n",
    "layer_norm = LayerNorm(normalized_shape)\n",
    "\n",
    "x = np.random.randn(10, 32, 32, 64)  # Example 4D tensor\n",
    "normalized_x = layer_norm(x)\n",
    "print(normalized_x.shape)  # Should be (10, 32, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['latents', 'projection.weight', 'projection.bias', 'layers.0.0.norm_media.weight', 'layers.0.0.norm_media.bias', 'layers.0.0.norm_latents.weight', 'layers.0.0.norm_latents.bias', 'layers.0.0.to_q.weight', 'layers.0.0.to_kv.weight', 'layers.0.0.to_out.weight', 'layers.0.1.0.weight', 'layers.0.1.0.bias', 'layers.0.1.1.weight', 'layers.0.1.3.weight', 'layers.1.0.norm_media.weight', 'layers.1.0.norm_media.bias', 'layers.1.0.norm_latents.weight', 'layers.1.0.norm_latents.bias', 'layers.1.0.to_q.weight', 'layers.1.0.to_kv.weight', 'layers.1.0.to_out.weight', 'layers.1.1.0.weight', 'layers.1.1.0.bias', 'layers.1.1.1.weight', 'layers.1.1.3.weight', 'layers.2.0.norm_media.weight', 'layers.2.0.norm_media.bias', 'layers.2.0.norm_latents.weight', 'layers.2.0.norm_latents.bias', 'layers.2.0.to_q.weight', 'layers.2.0.to_kv.weight', 'layers.2.0.to_out.weight', 'layers.2.1.0.weight', 'layers.2.1.0.bias', 'layers.2.1.1.weight', 'layers.2.1.3.weight', 'layers.3.0.norm_media.weight', 'layers.3.0.norm_media.bias', 'layers.3.0.norm_latents.weight', 'layers.3.0.norm_latents.bias', 'layers.3.0.to_q.weight', 'layers.3.0.to_kv.weight', 'layers.3.0.to_out.weight', 'layers.3.1.0.weight', 'layers.3.1.0.bias', 'layers.3.1.1.weight', 'layers.3.1.3.weight', 'layers.4.0.norm_media.weight', 'layers.4.0.norm_media.bias', 'layers.4.0.norm_latents.weight', 'layers.4.0.norm_latents.bias', 'layers.4.0.to_q.weight', 'layers.4.0.to_kv.weight', 'layers.4.0.to_out.weight', 'layers.4.1.0.weight', 'layers.4.1.0.bias', 'layers.4.1.1.weight', 'layers.4.1.3.weight', 'layers.5.0.norm_media.weight', 'layers.5.0.norm_media.bias', 'layers.5.0.norm_latents.weight', 'layers.5.0.norm_latents.bias', 'layers.5.0.to_q.weight', 'layers.5.0.to_kv.weight', 'layers.5.0.to_out.weight', 'layers.5.1.0.weight', 'layers.5.1.0.bias', 'layers.5.1.1.weight', 'layers.5.1.3.weight', 'norm.weight', 'norm.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1152])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['latents'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1152])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['latents'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536, 1152]), torch.Size([3072, 1152]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['layers.0.0.to_q.weight'].shape, ckpt['layers.0.0.to_kv.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536, 1152]), torch.Size([1536, 1152]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['layers.0.0.to_kv.weight'].chunk(2, dim=0)[0].shape, ckpt['layers.0.0.to_kv.weight'].chunk(2, dim=0)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.norm_media.weight\n",
      "layers.0.0.norm_media.bias\n",
      "layers.0.0.norm_latents.weight\n",
      "layers.0.0.norm_latents.bias\n",
      "layers.0.0.to_q.weight\n",
      "layers.0.0.to_kv.weight\n",
      "layers.0.0.to_out.weight\n",
      "layers.0.1.0.weight\n",
      "layers.0.1.0.bias\n",
      "layers.0.1.1.weight\n",
      "layers.0.1.3.weight\n"
     ]
    }
   ],
   "source": [
    "for k in ckpt.keys():\n",
    "    if 'layers.0' in k:\n",
    "        print(k)\n",
    "        \n",
    "def _replace_attn_layer(key, value):\n",
    "    # Check for the special case first\n",
    "    if re.match(r'layers\\.(\\d+)\\.0\\.to_kv\\.weight', key):\n",
    "        idx = re.search(r'layers\\.(\\d+)\\.0\\.to_kv\\.weight', key).group(1)\n",
    "        KVweight = value.chunk(2, dim=0)\n",
    "        return {f'blk.{idx}.attn.to_k.weight': KVweight[0],\n",
    "                f'blk.{idx}.attn.to_v.weight': KVweight[1]\n",
    "                }\n",
    "    \n",
    "    # Apply general replacements for other patterns\n",
    "    # Define the replacement patterns\n",
    "    patterns = [\n",
    "        (r'layers\\.(\\d+)\\.0\\.norm_media\\.(weight|bias)', r'blk.\\1.attn.norm_media.\\2'),\n",
    "        (r'layers\\.(\\d+)\\.0\\.norm_latents\\.(weight|bias)', r'blk.\\1.attn.norm_latents.\\2'),\n",
    "        (r'layers\\.(\\d+)\\.0\\.to_q\\.(weight)', r'blk.\\1.attn.to_q.\\2'),\n",
    "        (r'layers\\.(\\d+)\\.0\\.to_out\\.(weight)', r'blk.\\1.attn.to_out.\\2'),\n",
    "        (r'layers\\.(\\d+)\\.1\\.0\\.(weight|bias)', r'blk.\\1.ffn.ln.\\2'),\n",
    "        (r'layers\\.(\\d+)\\.1\\.1\\.weight', r'blk.\\1.ffn.linear_up.weight'),\n",
    "        (r'layers\\.(\\d+)\\.1\\.3\\.weight', r'blk.\\1.ffn.linear_down.weight'),\n",
    "    ]\n",
    "    for pattern, replacement in patterns:\n",
    "        key = re.sub(pattern, replacement, key)\n",
    "    \n",
    "    return {key: value}\n",
    "\n",
    "def replace_tensor_name_xgenmm_projector(ckpt):\n",
    "    identifier = 'perceiver_resampler.'\n",
    "    new_state_dict = {}\n",
    "    for k, v in ckpt.items():\n",
    "        # handel the layer\n",
    "        if 'layers' in k:\n",
    "            new_kvs = _replace_attn_layer(k, v)\n",
    "            for new_k, new_v in new_kvs.items():\n",
    "                new_state_dict[identifier+new_k] = new_v\n",
    "        elif k == 'norm.weight':\n",
    "            new_k = 'ln.weight'\n",
    "            new_state_dict[identifier+new_k] = v\n",
    "        elif k == 'norm.bias':\n",
    "            new_k = 'ln.bias'\n",
    "            new_state_dict[identifier+new_k] = v  \n",
    "        else:\n",
    "            new_state_dict[identifier+k] = v\n",
    "    return new_state_dict     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = replace_tensor_name_xgenmm_projector(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['perceiver_resampler.latents',\n",
       " 'perceiver_resampler.projection.weight',\n",
       " 'perceiver_resampler.projection.bias',\n",
       " 'perceiver_resampler.blk.0.attn.norm_media.weight',\n",
       " 'perceiver_resampler.blk.0.attn.norm_media.bias',\n",
       " 'perceiver_resampler.blk.0.attn.norm_latents.weight',\n",
       " 'perceiver_resampler.blk.0.attn.norm_latents.bias',\n",
       " 'perceiver_resampler.blk.0.attn.to_q.weight',\n",
       " 'perceiver_resampler.blk.0.attn.to_k.weight',\n",
       " 'perceiver_resampler.blk.0.attn.to_v.weight',\n",
       " 'perceiver_resampler.blk.0.attn.to_out.weight',\n",
       " 'perceiver_resampler.blk.0.ffn.ln.weight',\n",
       " 'perceiver_resampler.blk.0.ffn.ln.bias',\n",
       " 'perceiver_resampler.blk.0.ffn.linear_up.weight',\n",
       " 'perceiver_resampler.blk.0.ffn.linear_down.weight',\n",
       " 'perceiver_resampler.blk.1.attn.norm_media.weight',\n",
       " 'perceiver_resampler.blk.1.attn.norm_media.bias',\n",
       " 'perceiver_resampler.blk.1.attn.norm_latents.weight',\n",
       " 'perceiver_resampler.blk.1.attn.norm_latents.bias',\n",
       " 'perceiver_resampler.blk.1.attn.to_q.weight',\n",
       " 'perceiver_resampler.blk.1.attn.to_k.weight',\n",
       " 'perceiver_resampler.blk.1.attn.to_v.weight',\n",
       " 'perceiver_resampler.blk.1.attn.to_out.weight',\n",
       " 'perceiver_resampler.blk.1.ffn.ln.weight',\n",
       " 'perceiver_resampler.blk.1.ffn.ln.bias',\n",
       " 'perceiver_resampler.blk.1.ffn.linear_up.weight',\n",
       " 'perceiver_resampler.blk.1.ffn.linear_down.weight',\n",
       " 'perceiver_resampler.blk.2.attn.norm_media.weight',\n",
       " 'perceiver_resampler.blk.2.attn.norm_media.bias',\n",
       " 'perceiver_resampler.blk.2.attn.norm_latents.weight',\n",
       " 'perceiver_resampler.blk.2.attn.norm_latents.bias',\n",
       " 'perceiver_resampler.blk.2.attn.to_q.weight',\n",
       " 'perceiver_resampler.blk.2.attn.to_k.weight',\n",
       " 'perceiver_resampler.blk.2.attn.to_v.weight',\n",
       " 'perceiver_resampler.blk.2.attn.to_out.weight',\n",
       " 'perceiver_resampler.blk.2.ffn.ln.weight',\n",
       " 'perceiver_resampler.blk.2.ffn.ln.bias',\n",
       " 'perceiver_resampler.blk.2.ffn.linear_up.weight',\n",
       " 'perceiver_resampler.blk.2.ffn.linear_down.weight',\n",
       " 'perceiver_resampler.blk.3.attn.norm_media.weight',\n",
       " 'perceiver_resampler.blk.3.attn.norm_media.bias',\n",
       " 'perceiver_resampler.blk.3.attn.norm_latents.weight',\n",
       " 'perceiver_resampler.blk.3.attn.norm_latents.bias',\n",
       " 'perceiver_resampler.blk.3.attn.to_q.weight',\n",
       " 'perceiver_resampler.blk.3.attn.to_k.weight',\n",
       " 'perceiver_resampler.blk.3.attn.to_v.weight',\n",
       " 'perceiver_resampler.blk.3.attn.to_out.weight',\n",
       " 'perceiver_resampler.blk.3.ffn.ln.weight',\n",
       " 'perceiver_resampler.blk.3.ffn.ln.bias',\n",
       " 'perceiver_resampler.blk.3.ffn.linear_up.weight',\n",
       " 'perceiver_resampler.blk.3.ffn.linear_down.weight',\n",
       " 'perceiver_resampler.blk.4.attn.norm_media.weight',\n",
       " 'perceiver_resampler.blk.4.attn.norm_media.bias',\n",
       " 'perceiver_resampler.blk.4.attn.norm_latents.weight',\n",
       " 'perceiver_resampler.blk.4.attn.norm_latents.bias',\n",
       " 'perceiver_resampler.blk.4.attn.to_q.weight',\n",
       " 'perceiver_resampler.blk.4.attn.to_k.weight',\n",
       " 'perceiver_resampler.blk.4.attn.to_v.weight',\n",
       " 'perceiver_resampler.blk.4.attn.to_out.weight',\n",
       " 'perceiver_resampler.blk.4.ffn.ln.weight',\n",
       " 'perceiver_resampler.blk.4.ffn.ln.bias',\n",
       " 'perceiver_resampler.blk.4.ffn.linear_up.weight',\n",
       " 'perceiver_resampler.blk.4.ffn.linear_down.weight',\n",
       " 'perceiver_resampler.blk.5.attn.norm_media.weight',\n",
       " 'perceiver_resampler.blk.5.attn.norm_media.bias',\n",
       " 'perceiver_resampler.blk.5.attn.norm_latents.weight',\n",
       " 'perceiver_resampler.blk.5.attn.norm_latents.bias',\n",
       " 'perceiver_resampler.blk.5.attn.to_q.weight',\n",
       " 'perceiver_resampler.blk.5.attn.to_k.weight',\n",
       " 'perceiver_resampler.blk.5.attn.to_v.weight',\n",
       " 'perceiver_resampler.blk.5.attn.to_out.weight',\n",
       " 'perceiver_resampler.blk.5.ffn.ln.weight',\n",
       " 'perceiver_resampler.blk.5.ffn.ln.bias',\n",
       " 'perceiver_resampler.blk.5.ffn.linear_up.weight',\n",
       " 'perceiver_resampler.blk.5.ffn.linear_down.weight',\n",
       " 'perceiver_resampler.ln.weight',\n",
       " 'perceiver_resampler.ln.bias']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(projector.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05494d31407541bfa3db8f4eb455df9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213aa24ea7ef48c29cefa8a6b0a0cc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_minicpm.py:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2:\n",
      "- configuration_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a37bed5d814eed8eeea41caee8b88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_minicpmv.py:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697157f53323482089fcfd4109385476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resampler.py:   0%|          | 0.00/36.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2:\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1903bed8794551b0d55d0adc12001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_minicpm.py:   0%|          | 0.00/71.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2:\n",
      "- modeling_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2:\n",
      "- modeling_minicpmv.py\n",
      "- resampler.py\n",
      "- modeling_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982e9ac1698544b6b0e71da45698aa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f1e771a0b04575ad8269ebfce0b2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f894ab4b037470a88e4809ac2d20c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162978caf34c45f4b94dda25c2cbc7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f9dfef799646868980ad10fb1be99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e43882325b64e52874ff182dace14d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2', trust_remote_code=True)\n",
    "resampler = model.resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampler.pos_embed\n",
      "dict_keys(['resampler.pos_embed', 'resampler.pos_embed_k'])\n",
      "===\n",
      "resampler.query\n",
      "dict_keys(['resampler.query'])\n",
      "===\n",
      "resampler.proj\n",
      "dict_keys(['resampler.pos_embed_k', 'resampler.proj.weight'])\n",
      "===\n",
      "resampler.kv_proj.weight\n",
      "dict_keys(['resampler.kv_proj.weight'])\n",
      "===\n",
      "resampler.attn.in_proj_weight\n",
      "dict_keys(['resampler.attn.q.weight', 'resampler.attn.k.weight', 'resampler.attn.v.weight'])\n",
      "===\n",
      "resampler.attn.in_proj_bias\n",
      "dict_keys(['resampler.attn.q.bias', 'resampler.attn.k.bias', 'resampler.attn.v.bias'])\n",
      "===\n",
      "resampler.attn.out_proj.weight\n",
      "dict_keys(['resampler.attn.out_proj.weight'])\n",
      "===\n",
      "resampler.attn.out_proj.bias\n",
      "dict_keys(['resampler.attn.out_proj.bias'])\n",
      "===\n",
      "resampler.ln_q.weight\n",
      "dict_keys(['resampler.ln_q.weight'])\n",
      "===\n",
      "resampler.ln_q.bias\n",
      "dict_keys(['resampler.ln_q.bias'])\n",
      "===\n",
      "resampler.ln_kv.weight\n",
      "dict_keys(['resampler.ln_kv.weight'])\n",
      "===\n",
      "resampler.ln_kv.bias\n",
      "dict_keys(['resampler.ln_kv.bias'])\n",
      "===\n",
      "resampler.ln_post.weight\n",
      "dict_keys(['resampler.ln_post.weight'])\n",
      "===\n",
      "resampler.ln_post.bias\n",
      "dict_keys(['resampler.ln_post.bias'])\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _replace_name_resampler(s, v):\n",
    "    if re.match(\"resampler.pos_embed\", s):\n",
    "        return {\n",
    "            s: v,\n",
    "            re.sub(\"pos_embed\", \"pos_embed_k\", s): None,\n",
    "        }\n",
    "    if re.match(\"resampler.proj\", s):\n",
    "        return {\n",
    "            re.sub(\"proj\", \"pos_embed_k\", s): None, \n",
    "            re.sub(\"proj\", \"proj.weight\", s): v.transpose(-1, -2).contiguous(),\n",
    "        }\n",
    "    if re.match(\"resampler.attn.in_proj_.*\", s):\n",
    "        return {\n",
    "            re.sub(\"attn.in_proj_\", \"attn.q.\", s): v.chunk(3, dim=0)[0],\n",
    "            re.sub(\"attn.in_proj_\", \"attn.k.\", s): v.chunk(3, dim=0)[1],\n",
    "            re.sub(\"attn.in_proj_\", \"attn.v.\", s): v.chunk(3, dim=0)[2],\n",
    "        }\n",
    "    return {s: v}\n",
    "\n",
    "res = {}\n",
    "for k in model.state_dict().keys():\n",
    "    if re.match(\"resampler\", k):\n",
    "        print(k)\n",
    "        temp = _replace_name_resampler(k, model.state_dict()[k])\n",
    "        res.update(temp)\n",
    "        print(temp.keys())\n",
    "        print('===')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2304, 2304]), torch.Size([2304, 2304]), torch.Size([2304, 2304]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['resampler.attn.q.weight'].shape, res['resampler.attn.k.weight'].shape, res['resampler.attn.v.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgenmm-flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
